\documentclass[10pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{lmodern}
\usepackage{textcomp} % para símbolos como º
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{float}
\usepackage{microtype}

\lstdefinestyle{CStyle}{
  language=C,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=5pt,
  frame=single,
  breaklines=true,
  showstringspaces=false
}

\title{\textbf{Project Assignment II: Ecosystem Simulation}}
\author{Kathleen Soares\\ \texttt{up201903010}
\and Sérgio Cardoso\\ \texttt{up202107918}}
\date{\today}

\begin{document}
\maketitle
\section{Introduction}

This report describes the implementation and evaluation of a parallel ecosystem simulator developed for Project Assignment II of the Parallel Computing course. The objective was to model interactions between rabbits, foxes and rocks on a 2D grid and to analyse the effect of introducing OpenMP-based parallelism. The document provides a concise overview of the algorithm, the adopted parallelisation strategy, performance results, and the main difficulties encountered during development.

\section{Algorithm Overview}

The simulator operates on an $R \times C$ grid where each cell may contain a rabbit, fox, rock, or be empty. The simulation evolves for $\text{N\_GEN}$ generations, each consisting of two phases executed sequentially:

{\raggedright
\begin{itemize}
    \item \textbf{Rabbit:} move to adjacent empty cells. Reproduction occurs after a fixed number of generations (\texttt{GEN\_PROC\_RABBITS}) and rabbits left one child in the cell they vacated. After reproduction and leaving cell, the age of counter resets.
    \item \textbf{Fox:} move preferentially towards adjacent rabbits; otherwise they move to empty cells. They reproduce after \texttt{GEN\_PROC\_FOXES} generations and die if they do not eat for \texttt{GEN\_FOOD\_FOXES} generations. Foxes also leave one child in the cell they vacated upon reproduction, resetting their age counter.
\end{itemize}
}

Two grids (\texttt{grid1} and \texttt{grid2}) are used in a double-buffering scheme. Each cell stores the entity type and age counters. Conflict resolution uses a simple priority rule: older animals, or (for foxes) animals with lower food age, overwrite younger ones.

\subsection*{Conflicts resolution rules}

When multiple animals attempt to move into the same cell, the following rules are applied:
\begin{itemize}
    \item \textbf{Rabbits:} the rabbit with the highest procreation age (\texttt{PROC\_AGE}) prevails. Younger rabbits lose the conflict and are not placed in the target cell.
    
    \item \textbf{Foxes:} if multiple foxes attempt to occupy the same cell, the fox with the highest procreation age prevails. In case of a tie, the fox with the lowest food age (\texttt{FOOD\_AGE}) is selected, reflecting preferential survival of less-starved individuals.
\end{itemize}

In each generation, every animal increases its procreation age (\texttt{PROC\_AGE}) by one unit. For foxes, the food age (\texttt{FOOD\_AGE}) also increases by one when they do not eat during that generation.

\section{Parallel Implementation}

Parallelisation was introduced using OpenMP. A single parallel region encloses the full simulation, and parallel work is expressed via \texttt{\#pragma omp for}. Each generation includes:

\begin{itemize}
    \item Initialisation of the output grid.
    \item Processing of all rabbits or foxes using \texttt{schedule(guided)}.
\end{itemize}

\subsection{Synchronization}

Multiple animals may attempt to move into the same cell. To ensure correctness, the parallel version uses a lock table of 65\,536 OpenMP locks, mapped to grid indices via a bit mask. Each potentially conflicting update to the output grid acquires the corresponding lock, applies the conflict-resolution rules, and then releases the lock. Although correct, this introduces substantial synchronization overhead.

\subsection{Load Balancing}

The cost per cell depends on the presence of animals and their movement possibilities. To mitigate imbalance, \texttt{schedule(guided)} is used, providing dynamic chunk allocation with low overhead. Despite this, the inherent serialization between the rabbit and fox phases, together with fine-grained locking, limits scalability.

\section{Performance Evaluation}

\subsection{Execution Times and Speedups}

This section presents the execution times (in milliseconds) and the speedups obtained for the sequential version and for the parallel implementation using 1, 2, 4, 8, and 16 workers.

The average sequential execution time measured was $T_{\text{seq}} = 2767.548$ ms.  
Parallel execution times for each worker configuration were also collected and used to compute two types of speedup:

\begin{itemize}
    \item \textbf{Sequential speedup:} 
    \[
        S_{\text{seq}}(p) = \frac{T_{\text{seq}}}{T_p}
    \]
    which compares each parallel configuration with the sequential execution.
    
    \item \textbf{Relative speedup matrix:}
    \[
        S_i(p) = \frac{T_i}{T_p}
    \]
    which compares every worker configuration $p$ against a base configuration $i$.  
\end{itemize}

\begin{table}[H]
\centering
\caption{Speedup matrix comparing execution times between all worker configurations.}
\label{tab:speedup-matrix}
\begin{tabular}{c|ccccc}
\hline
\textbf{Base Workers $i$} & \textbf{1} & \textbf{2} & \textbf{4} & \textbf{8} & \textbf{16} \\
\hline
1  & 1.000 & 1.410 & 2.227 & 3.192 & 3.558 \\
2  & 0.708 & 1.000 & 1.578 & 2.262 & 2.522 \\
4  & 0.448 & 0.633 & 1.000 & 1.433 & 1.597 \\
8  & 0.313 & 0.441 & 0.697 & 1.000 & 1.114 \\
16 & 0.280 & 0.396 & 0.625 & 0.896 & 1.000 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Speedup in relation to the sequential execution time.}
\label{tab:seq-speedup}
\begin{tabular}{c|cc}
\hline
\textbf{Workers $p$} & \textbf{Execution Time $T_p$ (ms)} & \textbf{Speedup $S_{\text{seq}}(p)$} \\
\hline
1  & 4781.816 & 0.578 \\
2  & 3389.943 & 0.816 \\
4  & 2146.978 & 1.289 \\
8  & 1498.052 & 1.847 \\
16 & 1343.618 & 2.059 \\
\hline
\end{tabular}
\end{table}

Table~\ref{tab:speedup-matrix} shows that the parallel version exhibits internal scalability: configurations with more workers outperform those with fewer workers. However, when compared with the sequential runtime ($T_{\text{seq}}$), all parallel configurations remain slower, as shown in Table~\ref{tab:seq-speedup}. This indicates that the overhead introduced by parallelisation dominates the useful computation.

Although the parallel implementation becomes faster as more threads are added, it never surpasses the sequential version. This occurs because parallel execution introduces substantial overhead --- including lock contention, memory-coherence traffic, and synchronization barriers --- which outweighs the useful work performed.

\subsection{Analysis of Results}
The main causes of this lack of scalability are:

\begin{itemize}
    \item \textbf{High synchronization overhead:}  
    The fine-grained lock table serializes a large portion of the updates, causing threads to wait frequently.
    
    \item \textbf{Memory contention:}  
    Both grids are accessed intensively by all workers, leading to cache invalidations and increased memory-coherence traffic.
    
    \item \textbf{Low computational granularity:}  
    Each update performs only a small amount of useful work, while lock acquisition and release dominate the total cost.
    
    \item \textbf{Phase dependency:}  
    The strict ordering between the rabbit and fox phases prevents additional parallelism and limits speedup.
\end{itemize}

Even though the relative speedup matrix shows that using more workers can improve performance when compared with fewer workers, the \textbf{absolute performance remains worse than the sequential version for all thread counts}.

\subsection{Comparison with Previous Measurements}

\begin{table}[H]
\centering
\caption{Comparison between an locally machine and hyrax64 sequential speedup values.}
\label{tab:old-vs-new-speedup}
\begin{tabular}{c|cc}
\hline
\textbf{Workers $p$} & \textbf{locally $S_{\text{seq}}(p)$} & \textbf{hyrax64 $S_{\text{seq}}(p)$} \\
\hline
1  & 0.553 & 0.578 \\
2  & 0.794 & 0.816 \\
4  & 0.911 & 1.289 \\
8  & 0.633 & 1.847 \\
16 & 0.416 & 2.059 \\
\hline
\end{tabular}
\end{table}

To contextualize the updated results obtained on \texttt{hyrax64}, Table~\ref{tab:old-vs-new-speedup} compares the new speedup values with the previous measurements taken locally. The earlier results showed performance degradation as the number of workers increased, while the updated results exhibit clear internal scalability and noticeably higher speedups.

Overall, the comparison indicates that the previous execution environment suffered from severe contention, higher synchronization costs, and poor scaling, while the \texttt{hyrax64} environment provides substantially better thread utilization.

The differences between the two sets of results arise from variations in system architecture and runtime conditions. The earlier machine exhibited stronger effects from lock contention, memory bandwidth limitations, and cache interference, which severely hindered scalability. In contrast, \texttt{hyrax64} offers more efficient parallel execution, improved cache performance, and lower contention, allowing the algorithm to achieve significantly better speedups and more consistent internal scalability.

\section{Difficulties and Comments}

The main difficulty was ensuring the correctness of the ecosystem rules. Small inaccuracies in movement or reproduction generated noticeable differences, requiring iterative debugging.

The parallel version introduced the most challenges. Frequent concurrent accesses to the grid created write conflicts, making lock-based synchronization necessary. However, the resulting overhead dominated execution time and prevented speedup. Understanding these effects and tuning the implementation were important parts of the development process.

The project illustrated that parallelisation is not always beneficial, especially for algorithms with fine-grained operations and extensive shared-memory interaction. Future work could explore alternative decompositions or different parallel programming models with lower contention.

\section{Conclusion}

Although multiple thread configurations were tested (1, 2, 4, 8, and 16 workers), none of them outperformed the sequential execution. The highest speedup obtained relative to the sequential baseline was $S_{\text{seq}}(16)=2.059$, which remains below~1 and confirms that the overhead introduced by parallelisation outweighs the useful computational work.

The speedup matrix shows that increasing the number of workers provides modest internal improvements (e.g., $S_{8}(16)=1.114$). However, these gains are not sufficient to overcome synchronization costs, memory contention, and the fine-grained nature of the updates. This demonstrates that, for algorithms dominated by shared-memory conflicts and frequent synchronization, OpenMP parallelisation may hinder performance rather than enhance it.

\begin{thebibliography}{9}

\bibitem{quinn}
M. J. Quinn, \textit{Parallel Programming in C with MPI and OpenMP}. McGraw-Hill, 2003.

\bibitem{chandra}
R. Chandra, L. Dagum, D. Kohr, D. Maydan, J. McDonald and R. Menon,
\textit{Parallel Programming in OpenMP}. Morgan Kaufmann, 2000.

\bibitem{openmp}
OpenMP Architecture Review Board,
\textit{OpenMP Application Programming Interface}. Version 5.2, 2023.
Available at: \url{https://www.openmp.org/specifications/}

\end{thebibliography}

\textit{Note: The complete source code (\texttt{ecosystem.c} and \texttt{ecosystem\_seq.c}) is attached in the ZIP file submitted along with this report.}

\end{document}